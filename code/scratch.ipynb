{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"nuoyth0cRRRe"},"source":["part 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3152,"status":"ok","timestamp":1684655002619,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"ahStIDy8PuCJ","outputId":"f67765d1-46ca-4b8f-a903-57f2f7242eeb"},"outputs":[],"source":["import requests\n","import os\n","import gzip\n","import pandas as pd\n","\n","# download crux dataset (if needed) and read into a dataframe\n","def get_dataset(fname='current.csv'):\n","  if not os.path.isfile(fname):\n","    print('downloading data...')\n","    url = 'https://raw.githubusercontent.com/zakird/crux-top-lists/main/data/global/current.csv.gz'\n","    response = requests.get(url)\n","\n","    # write download contents to 'current.csv.gz'\n","    gzip.open(f'{fname}.gz', 'wb').write(response.content)\n","    # unzip to get 'current.csv'\n","    os.system(f'gzip -d {fname}.gz')\n","\n","  try:\n","    df = pd.read_csv(fname, compression='gzip')\n","  except:\n","    df = pd.read_csv(fname)\n","  return df\n","\n","\n","# get dataframe\n","df = get_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684655002620,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"ipvVj1iHOb7A","outputId":"e7fd863f-353e-413a-d34b-4f50c93d0dc0"},"outputs":[],"source":["import random\n","random.seed()\n","\n","# returns a new dataframe of only websites with specified rank\n","def rank_filter(df, rank):\n","  return df[df['rank'] == rank]\n","\n","# extract a random sample of n urls\n","def get_sample(df, n, save=None):\n","  N = len(df) # population size\n","  n = 100     # sample size\n","  sample_idxs = random.sample(range(N), n)\n","  sample_urls = [df['origin'].iloc[i] for i in sample_idxs]\n","\n","  # if save parameter is provided, save urls to specified output file\n","  if save:\n","    open(save, 'w').write('\\n'.join(sample_urls))\n","  return sample_urls\n","\n","\n","# get rank 1000 websites\n","df1000 = rank_filter(df, 1000)\n","print(df1000)\n","\n","# sample of urls\n","urls = get_sample(df, 100, save='urls.txt')\n","urls1000 = get_sample(df1000, 100, save='urls1000.txt')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hWF-wdeYRQE_"},"source":["part 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34202,"status":"ok","timestamp":1684655153237,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"dSwmN_3rPSIZ","outputId":"ab9ed9fc-dcd2-4f89-c085-94fe9daa0c62"},"outputs":[],"source":["%pip install beautifulsoup4 \n","%pip install google\n","\n","from googlesearch import search\n","from tqdm import tqdm\n","\n","def print_search_results(url, result):\n","  print(url)\n","  print(result)\n","  print()\n","\n","# take top google search results and see if any contain an aup\n","# search-width: n\n","def get_search_results(url, condition=(lambda x, y: 'aup' in y or 'acceptable-use-policy' in y), n=5):\n","  search_str = f'{url} acceptable use policy'\n","  for result in search(search_str, num=n, stop=n, pause=1):\n","    if condition(url, result):\n","      return result\n","  return None\n","\n","\n","# example\n","aups = set()\n","for url in urls[:5]:\n","  result = get_search_results(url)\n","  if result:\n","    aups.add(result)\n","    print_search_results(url, result)\n","\n","print(f'{len(aups)} results found')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":166,"status":"ok","timestamp":1684655186611,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"aBKXhCQbGGDI"},"outputs":[],"source":["import csv \n","\n","# get aup's in the bucket specified by a given rank\n","# don't worry about duplicates here, remove them later in a dataframe\n","def get_aups_in_bucket(partial_df):\n","  aups = []\n","  rank = partial_df.iloc[0]['rank']\n","\n","  # create a new file to store aup urls\n","  with open(f'aups-rank{rank}.csv', 'w') as f:\n","    w = csv.writer(f)\n","    w.writerow(['index', 'aup'])\n","\n","  for i in tqdm(range(len(partial_df))):\n","    row = partial_df.iloc[i]\n","    idx = row['index']\n","    url = row['origin']\n","       \n","    # run google search for relevant aup\n","    result = get_search_results(url)\n","    if result:\n","      aups.append((idx, result))\n","    \n","    # print progress update and write to file after every 1000 searches\n","    if i % 1000 == 0 and i != 0:\n","      print(f' found {len(aups)} new aups')\n","      with open(f'aups-rank{rank}.csv', 'a') as f:\n","        w = csv.writer(f)\n","        w.writerows(aups)\n","      aups = []"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zXkEp0cuROYJ"},"source":["part 3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":817},"executionInfo":{"elapsed":132287,"status":"error","timestamp":1684655471378,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"AH-qj1KhQMuk","outputId":"5697be57-a253-43cf-f109-7ba2ed13dc0b"},"outputs":[],"source":["# query the Wayback Availability JSON API\n","# limitation: only returns a single snapshot (most recent or closest to query timestamp)\n","def query_wayback(url):\n","  query = f'https://archive.org/wayback/available?url={url}'\n","  response = requests.get(query)\n","  return response.json()\n","\n","# query the Wayback CDX Server API\n","# this provides more complex support, including returning all available snapshots\n","def query_cdx(url):\n","  query = f'http://web.archive.org/cdx/search/cdx?url={url}&fl=timestamp,digest,length,original&output=json'\n","  response = requests.get(query)\n","  return response.json()\n","\n","\n","# example query results\n","aups = list(aups)\n","x = aups[0]\n","print(query_wayback(x))\n","print('\\n'.join(list(map(str, query_cdx(x)))))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"elapsed":8066,"status":"error","timestamp":1684655523760,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"-8KXX2AYRKrC","outputId":"6ea6bb14-ab16-45ca-9d05-fd7de732eb90"},"outputs":[],"source":["# get urls of all wayback machine snapshots from a current url\n","def get_snapshots(url):\n","  cdx = query_cdx(url)[1:] # skip header\n","\n","  # c[0] is timestamp and c[3] is original url\n","  snapshots = [f'http://web.archive.org/web/{c[0]}/{c[3]}' for c in cdx]\n","  timestamps = [c[0] for c in cdx]\n","  return snapshots, timestamps\n","\n","\n","# example\n","snapshots, _ = get_snapshots(x)\n","print('\\n'.join(snapshots))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Agaich7cRMes"},"source":["part 4"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":780},"executionInfo":{"elapsed":21570,"status":"error","timestamp":1684655945133,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"jBVLZtNsQSvi","outputId":"6ba0ef9a-c2d6-43df-b8d2-a1f48e26ae5c"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.3.1 -> 23.1.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: PyPDF2 in c:\\users\\kyeling\\desktop\\cse256\\project\\cse256-aup-project\\.venv\\lib\\site-packages (3.0.1)\n"]}],"source":["%pip install PyPDF2\n","\n","from bs4 import BeautifulSoup\n","import requests\n","from PyPDF2 import PdfReader\n","from io import BytesIO\n","import random # TODO remove later\n","\n","## below code can be replaced by using requests\n","# from urllib.request import Request, urlopen\n","# import ssl\n","# ctx = ssl.create_default_context() # pass this into urlopen as context=ctx\n","# ctx.set_ciphers('DEFAULT')\n","\n","# aup = 'https://www.whitman.edu/technology-services/policies/acceptable-use-policy'\n","# aup = 'https://www.kean.edu/media/computer-related-acceptable-use-policy' # pdf... TT\n","\n","# combining part 3 and 4 to test scraping from Wayback Machine:\n","aup = 'http://web.archive.org/web/20230518110433/https://aws.amazon.com/aup/' # snapshot\n","index = random.randint(0,1000000)\n","\n","def get_aup_content(aup_url, index, timestamp='current'):\n","  response = requests.get(aup_url)\n","  padded_index = str(index).zfill(6)\n","  text = \"\"\n","\n","  if 'text/html' in response.headers['Content-Type']:\n","    soup = BeautifulSoup(response.content)\n","    elems_to_rm = ['style', 'script', 'head', 'title', 'nav', 'heater', 'footer', 'button', 'a']\n","    for s in soup(elems_to_rm):\n","      s.extract()\n","    text = soup.get_text()\n","\n","  elif 'pdf' in response.headers['Content-Type']:\n","    # src: https://wellsr.com/python/read-pdf-files-with-python-using-pypdf2/\n","    pdf_bytes = BytesIO(response.content)\n","    pdf = PdfReader(pdf_bytes)\n","    for page in pdf.pages:\n","      text += page.extract_text()\n","    \n","  else:\n","    print('urecognized content type', response.headers['Content-Type'])\n","    # TODO return error\n","\n","  # strip leading/trailing space and drop blank lines\n","  lines = (line.strip() for line in text.splitlines())\n","  cleantext = '\\n'.join(line for line in lines if line)\n","\n","  # write to file\n","  fname = f'../data/aups/{padded_index}-{timestamp}.txt'\n","  open(fname, 'w').write(cleantext)\n","\n","\n","# example\n","get_aup_content(aup, index)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"voURHWNJHTfX"},"source":["### miscellaneous"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## scraping urls with selenium\n","\n","from bs4 import BeautifulSoup\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.chrome.service import Service\n","from webdriver_manager.chrome import ChromeDriverManager\n","\n","options = Options()\n","options.binary_location = r'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'\n","\n","service = Service(ChromeDriverManager().install())\n","driver = webdriver.Chrome(service=service, options=options)\n","\n","search_str = 'acceptable use policy'\n","driver.get(f\"https://www.google.com/search?q={search_str}\")\n","\n","soup = BeautifulSoup(driver.page_source, 'html.parser')\n","\n","search = soup.find_all('div', class_=\"yuRUbf\")\n","for h in search:\n","    print(h.a.get('href'))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1684655111284,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"IVRVUOaCgbx2"},"outputs":[],"source":["## url parsing and string matching attempts D:\n","\n","import urllib.parse\n","import socket\n","\n","url = 'https://google.com'\n","\n","def get_ip(url):\n","  parsed_url = urllib.parse.urlparse(url)\n","  print(parsed_url.netloc)\n","  print(socket.gethostbyname(parsed_url.netloc))\n","\n","\n","# example\n","get_ip(url)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### master csv for final dataset"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import os\n","import csv\n","\n","datapath = 'data/final_data/'\n","cruxpath = f'../{datapath}crux/'\n","googlepath = f'../{datapath}googlesearch/'\n","github_url = f'https://github.com/kyeling/cse256-aup-project/tree/main/{datapath}'\n","\n","with open('../data/master.csv', newline='', mode='w') as f:\n","    w = csv.writer(f)\n","    w.writerow(['id', 'source', 'url', 'filepath', 'link-to-filepath', 'sector'])\n","\n","    crux_df = pd.read_csv('../data/aup-urls/crux-aups.csv')\n","    crux_df = crux_df.set_index('index')\n","    for fname in os.listdir(cruxpath):\n","        if fname == '.gitignore': continue\n","        id, _ = fname.split('-')\n","        url = crux_df.loc[int(id)]['aup']\n","        w.writerow([id, 'crux', url, fname, f'{github_url}{fname}'])\n","\n","    googlesearch_list = open('../data/aup-urls/googlesearch-aups.txt').readlines()\n","    for fname in os.listdir(googlepath):\n","        if fname == '.gitignore': continue\n","        id, _ = fname.split('-')\n","        url = googlesearch_list[int(id)].replace('\\n', '')\n","        w.writerow([id, 'googlesearch', url, fname, f'{github_url}{fname}'])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO6FtmmOkecg2DyoSu5yZxH","collapsed_sections":["voURHWNJHTfX"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
