{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "SuNjiwUbXZzD",
        "outputId": "95dfdd08-3103-44e7-ed42-1d7601a3f83e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import glob\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/AUP_project/AUPs/'\n",
        "files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
        "\n",
        "with open(file_path, 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "\n",
        "detected_encoding = result['encoding']\n",
        "\n",
        "text_data = []\n",
        "\n",
        "for file_path in files:\n",
        "    with open(file_path, 'r', encoding=detected_encoding) as f:\n",
        "        text = f.read()\n",
        "        text_data.append(text)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [lemmatizer.lemmatize(w) for w in word_tokens if w not in stop_words and w.isalpha()]\n",
        "    return filtered_text\n",
        "\n",
        "# Preprocess all the documents\n",
        "text_data = [preprocess_text(doc) for doc in text_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzvG_fDlQ85h",
        "outputId": "cf8b0d33-bdfa-40a8-e86c-0a14d2af98e9"
      },
      "outputs": [],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xMB_8wQQVkE"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Phrases\n",
        "\n",
        "# bigram model\n",
        "bigram = Phrases(text_data, min_count=5, threshold=100)\n",
        "\n",
        "# trigram model\n",
        "trigram = Phrases(bigram[text_data], threshold=100)\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram[bigram[doc]] for doc in texts]\n",
        "\n",
        "text_data = make_bigrams(text_data)\n",
        "text_data = make_trigrams(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi2XMv_MRNh4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join back the tokenized words\n",
        "text_data_joined = [' '.join(doc) for doc in text_data]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text_data_joined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezboPGFfQ53h",
        "outputId": "46df1c1f-9768-4129-86d3-aa601d8120fe"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "n_topics = 10\n",
        "nmf = NMF(n_components=n_topics, random_state=1)\n",
        "nmf.fit(X)\n",
        "\n",
        "def get_top_words(model, feature_names, n_top_words):\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topics.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "    return topics\n",
        "\n",
        "n_top_words = 10\n",
        "tf_feature_names = vectorizer.get_feature_names_out()\n",
        "topic_words = get_top_words(nmf, tf_feature_names, n_top_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPxjZcjIWHrh",
        "outputId": "efcbaa81-51fc-4bbc-aa91-34e0980e109c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "def contains_topic_word(sentence, topic_words):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    return any(word in topic_words for word in words)\n",
        "\n",
        "# Join the tokenized words back into sentences\n",
        "text_data_joined = [' '.join(doc) for doc in text_data]\n",
        "\n",
        "for i, words in enumerate(topic_words):\n",
        "    topic_sentences = [sentence for document in text_data_joined for sentence in nltk.sent_tokenize(document) if contains_topic_word(sentence, words)]\n",
        "    print(f\"Topic #{i}:\")\n",
        "    print(topic_sentences[:1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN0wpXqebk7u",
        "outputId": "9a43d771-56ad-4de6-836b-bdd7f3add7b9"
      },
      "outputs": [],
      "source": [
        "disallowed_indicator_words = [\"not\", \"prohibit\", \"ban\", \"disallow\", \"forbid\"]\n",
        "\n",
        "def contains_disallowed_indicator_word(sentence):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    return any(word in disallowed_indicator_words for word in words)\n",
        "\n",
        "disallowed_sentences = [sentence for document in text_data_joined for sentence in nltk.sent_tokenize(document) if contains_disallowed_indicator_word(sentence)]\n",
        "\n",
        "print(disallowed_sentences[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh_mWMX5ckXs",
        "outputId": "2f38cc3a-f6d0-4a3d-a7b5-c24627282850"
      },
      "outputs": [],
      "source": [
        "# process before training NMF\n",
        "disallowed_sentences = [sentence for document in text_data_joined for sentence in nltk.sent_tokenize(document) if contains_disallowed_indicator_word(sentence)]\n",
        "\n",
        "# train the NMF model on these sentences only\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_disallowed = vectorizer.fit_transform(disallowed_sentences)\n",
        "\n",
        "nmf_disallowed = NMF(n_components=n_topics, random_state=1)\n",
        "nmf_disallowed.fit(X_disallowed)\n",
        "\n",
        "tf_feature_names_disallowed = vectorizer.get_feature_names_out()\n",
        "topic_words_disallowed = get_top_words(nmf_disallowed, tf_feature_names_disallowed, n_top_words)\n",
        "\n",
        "topic_words_disallowed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cVsFMVJKfvR",
        "outputId": "8836448d-0520-4056-afb2-81e3c97baebb"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=100,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)\n",
        "\n",
        "lda.fit(X)\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % (topic_idx+1)\n",
        "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "\n",
        "n_top_words = 10\n",
        "tf_feature_names = vectorizer.get_feature_names_out()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBbPAIf4Ovzq",
        "outputId": "d10b1953-5031-4f1f-e748-e6bc2e75e656"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 10\n",
        "km = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1)\n",
        "km.fit(X)\n",
        "\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms[ind], end='')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA5NnQlBPrqW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "Nl4EEPc1do1x",
        "outputId": "da629f53-99b6-403a-ba74-9ed0b3a6762c"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2).fit(X.toarray())\n",
        "data2D = pca.transform(X.toarray())\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters).fit(data2D)\n",
        "clusters = km.labels_.tolist()\n",
        "\n",
        "# Plotting the cluster obtained using k means\n",
        "plt.figure(figsize=(10, 10))\n",
        "scatter = plt.scatter(data2D[:, 0], data2D[:, 1], c=km.labels_, s=50, cmap='viridis')\n",
        "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=300, c='red')\n",
        "plt.title('Clusters of documents')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gfcahmpUracz",
        "outputId": "e1dab372-c9e4-4bf2-fc0a-954331e26018"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def get_sector_prediction(url):\n",
        "    prompt = f\"Requirement: Please state the sector in the first word of your response. Question: Based on the URL '{url}', what would be the most likely sector for this business?\"\n",
        "\n",
        "    response = requests.post(\n",
        "        'https://api.openai.com/v1/completions',\n",
        "        headers={\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'Bearer',\n",
        "        },\n",
        "        json={\n",
        "            'model': 'text-davinci-003',\n",
        "            'prompt': prompt,\n",
        "            'max_tokens': 100,\n",
        "            'temperature': 0.5,\n",
        "        },\n",
        "    )\n",
        "    response_json = response.json()\n",
        "\n",
        "    try:\n",
        "        # Extract the first word from the response\n",
        "        first_word = response_json['choices'][0]['text'].split()[0]\n",
        "        return first_word\n",
        "    except KeyError:\n",
        "        print(response_json)  # If there's an error, print the response\n",
        "        return None\n",
        "\n",
        "def extract_sector_from_response(response):\n",
        "    sectors = ['Technology', 'Telecommunications', 'Healthcare', 'Finance', 'Education', 'Government', 'Retail', 'Entertainment']\n",
        "    detected_sectors = []\n",
        "\n",
        "    for sector in sectors:\n",
        "        if sector.lower() in response.lower():\n",
        "            detected_sectors.append(sector)\n",
        "\n",
        "    if not detected_sectors:\n",
        "        detected_sectors.append('Other')\n",
        "\n",
        "    return detected_sectors[0]\n",
        "\n",
        "\n",
        "# Load your CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/AUP_project/master_219.csv')\n",
        "\n",
        "sectors = []\n",
        "# Apply sector prediction for each URL\n",
        "for url in df['url']:\n",
        "    sector = get_sector_prediction(url)\n",
        "    sectors.append(sector)\n",
        "    print(f'URL: {url}, Predicted Sectors: {sector}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COcSyjItNn2F",
        "outputId": "a770809c-2660-4758-e25f-2d483d02990b"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2KySEOMZDYv",
        "outputId": "503f75f8-c909-4d4c-e776-4b89b267a436"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import chardet\n",
        "\n",
        "def truncate_text(text, max_length):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    truncated_text = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(truncated_text + sentence) <= max_length:\n",
        "            truncated_text += sentence\n",
        "        else:\n",
        "            break\n",
        "    return truncated_text\n",
        "\n",
        "# Set API key\n",
        "\n",
        "directory = '/content/drive/MyDrive/AUP_project/AUPs/'\n",
        "\n",
        "with open(file_path, 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "detected_encoding = result['encoding']\n",
        "\n",
        "# Loop over all text files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename in df['TextFile'].values:\n",
        "        continue\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(directory, filename), 'r', encoding=detected_encoding, errors='ignore') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Truncate the text to fit within the model's token limit (approximately 2048 tokens)\n",
        "        truncated_text = truncate_text(text, 2048)\n",
        "\n",
        "        # Define the prompt\n",
        "                # Define the prompt\n",
        "        prompt = f\"please identify every action that the website disallows users to do in the following acceptable user policy and give that action a category label (one to three words that briefly summarize the disallowed action). Please format your answer strictly in a CSV format where the two columns are: 'Disallowed Actions' and 'Label'.\\n\\n{truncated_text}\"\n",
        "\n",
        "        # Send the request to the GPT-3 model\n",
        "        response = requests.post(\n",
        "            'https://api.openai.com/v1/completions',\n",
        "            headers={\n",
        "                'Content-Type': 'application/json',\n",
        "                'Authorization': f'Bearer {api_key}',\n",
        "            },\n",
        "            json={\n",
        "                'model': 'text-davinci-003',\n",
        "                'prompt': prompt,\n",
        "                'max_tokens': 2048,\n",
        "                'temperature': 0.5,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Parse the response\n",
        "        response_data = response.json()['choices'][0]['text'].strip().split('\\n')\n",
        "\n",
        "        # Identify the start of the disallowed actions and labels\n",
        "        start_index = response_data.index('Disallowed Actions,Label') + 1\n",
        "\n",
        "        # Parse the disallowed actions and labels\n",
        "        for row in response_data[start_index:]:\n",
        "            if ',' in row:  # Only process rows that contain a comma\n",
        "                disallowed_action, label = row.split(',', 1)  # Split on the first comma only\n",
        "                df = df.append({\"TextFile\": filename, \"Disallowed Action\": disallowed_action.strip(), \"Label\": label.strip()}, ignore_index=True)\n",
        "# Save the dataframe to a CSV file\n",
        "df.to_csv('output.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex5Rjj05wKRb"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/drive/MyDrive/AUP_project/Raw_Disallows.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "YeIuyUeMf7uK",
        "outputId": "02441176-cd7e-4f35-8abe-bf11d473a766"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj6j6hqshyt4",
        "outputId": "57c7c474-2c10-4e6e-cfba-dde7fa6000b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.unique(np.array(df.Label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWn8gDhgkhKp",
        "outputId": "8f697039-71c3-4025-ba96-bd1283c79b3c"
      },
      "outputs": [],
      "source": [
        "np.unique(np.array(df.TextFile))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLWMYmkgaZKn",
        "outputId": "41a8b2e1-0778-456d-fe55-ec683658d993"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lF0Xie_30CA",
        "outputId": "0edfe975-22ff-4960-d40a-c96afaa3e969"
      },
      "outputs": [],
      "source": [
        "label_to_category = {\n",
        "    # Abuse and Harassment\n",
        "    'Abuse': 'Abuse and Harassment',\n",
        "    'Abuse of Resources': 'Abuse and Harassment',\n",
        "    'Abusive': 'Abuse and Harassment',\n",
        "    'Abusive Behavior': 'Abuse and Harassment',\n",
        "    'Abusive Content': 'Abuse and Harassment',\n",
        "    'Bullying': 'Abuse and Harassment',\n",
        "    'Harassing': 'Abuse and Harassment',\n",
        "    'Harassing Content': 'Abuse and Harassment',\n",
        "    'Harassment': 'Abuse and Harassment',\n",
        "    'Harassment/Bullying': 'Abuse and Harassment',\n",
        "    'Harassment/Intimidation': 'Abuse and Harassment',\n",
        "    'Hate': 'Abuse and Harassment',\n",
        "    'Hate Speech': 'Abuse and Harassment',\n",
        "    'Hateful': 'Abuse and Harassment',\n",
        "    'Hateful content': 'Abuse and Harassment',\n",
        "    # And so on...\n",
        "\n",
        "    # Illegal Activities\n",
        "    'Illegal': 'Illegal Activities',\n",
        "    'Illegal Actions': 'Illegal Activities',\n",
        "    'Illegal Activities': 'Illegal Activities',\n",
        "    'Illegal Activity': 'Illegal Activities',\n",
        "    'Illegal Content': 'Illegal Activities',\n",
        "    'Illegal Domain Names': 'Illegal Activities',\n",
        "    'Illegal Use': 'Illegal Activities',\n",
        "    'Illegal activities': 'Illegal Activities',\n",
        "    'Illegal activity': 'Illegal Activities',\n",
        "    'Illegal behavior': 'Illegal Activities',\n",
        "    'Illegal content': 'Illegal Activities',\n",
        "    'Illegal material': 'Illegal Activities',\n",
        "    'Illegal/Fraudulent': 'Illegal Activities',\n",
        "    'Illegal/Fraud': 'Illegal Activities',\n",
        "    'Law violation': 'Illegal Activities',\n",
        "    'Law-breaking': 'Illegal Activities',\n",
        "    'Lawbreaking': 'Illegal Activities',\n",
        "\n",
        "    # Content Issues\n",
        "    'Adult Content': 'Content Issues',\n",
        "    'Adult Services': 'Content Issues',\n",
        "    'Content Removal': 'Content Issues',\n",
        "    'Content Standards': 'Content Issues',\n",
        "    # Continue with other labels for this category...\n",
        "\n",
        "    # Fraudulent Activities\n",
        "    'Fraud': 'Fraudulent Activities',\n",
        "    'Fraudulent': 'Fraudulent Activities',\n",
        "    'Fraudulent Activities': 'Fraudulent Activities',\n",
        "    'Fraudulent Interaction': 'Fraudulent Activities',\n",
        "    # Continue with other labels for this category...\n",
        "\n",
        "    # add other categories\n",
        "}\n",
        "\n",
        "df['New_Label'] = df['Label'].map(label_to_category)\n",
        "\n",
        "# Check for any labels that were not included in the label_to_category dictionary\n",
        "# assign a new_label value of NaN\n",
        "missing_labels = df['Label'][df['New_Label'].isnull()].unique()\n",
        "\n",
        "print('Labels not included in label_to_category:')\n",
        "for label in missing_labels:\n",
        "    print(label)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
