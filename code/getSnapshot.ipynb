{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9ljax76glzUh9xIyb5hC1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install beautifulsoup4 \n","!pip install google\n","!pip install PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aw0v_q6Fzhhq","executionInfo":{"status":"ok","timestamp":1686289384415,"user_tz":420,"elapsed":20570,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}},"outputId":"1ff1612c-3806-4de9-a25d-592c471896f7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google) (2.4.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5N5Ze_AjvJrA","executionInfo":{"status":"ok","timestamp":1686289385632,"user_tz":420,"elapsed":1227,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}}},"outputs":[],"source":["import data\n","import csv\n","import pandas as pd\n"]},{"cell_type":"code","source":["# read in txt data as list\n","def readInFile(filename = 'googlesearch-aups.txt'):\n","  with open(filename) as file:\n","      aup_data = [line.rstrip() for line in file]\n","\n","  return aup_data[1:] # get rid of the header"],"metadata":{"id":"eI5lD48AWZzQ","executionInfo":{"status":"ok","timestamp":1686289385632,"user_tz":420,"elapsed":12,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Url Collection"],"metadata":{"id":"U7W-NuZ0gtNZ"}},{"cell_type":"code","source":["# for each url we find their snapshots for the past 10 years, one for each year\n","def collectURL(aup_data):\n","  '''\n","  url_snapshots_dict: {\n","      url: (year, snapshot)\n","  }\n","  '''\n","  url_snapshots_dict = {}\n","\n","  for url in aup_data:\n","    try:\n","      snapshots, timestamps = data.get_snapshots(url)\n","      snapshot_within_10_years = []\n","      listed_years = [] # a list that keeps the recorded years since we only want 1 snapshot each year\n","      for i in range(len(timestamps)):\n","        year = int(timestamps[i][:4])\n","        if year in listed_years:\n","          continue\n","        # if the timestamp is within 10 years, append corresponding snapshot\n","        if year >= 2013 and year <= 2023:\n","          snapshot_within_10_years.append((year, snapshots[i]))\n","          listed_years.append(year)\n","      url_snapshots_dict[url] = snapshot_within_10_years\n","    except Exception as inst: \n","      print(f\"Exception when dealing with {url}\")\n","      print(inst)\n","      continue\n","  \n","  return url_snapshots_dict"],"metadata":{"id":"B80D7RUw0bZ1","executionInfo":{"status":"ok","timestamp":1686289419459,"user_tz":420,"elapsed":220,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def writeToCSV(url_snapshots_dict):\n","  try:\n","    df = pd.DataFrame(url_snapshots_dict.items(), columns=['aup-url', 'snapshots'])\n","    df.to_csv('snapshots_from_googlesearch.csv', index=False, header=True)\n","    return True\n","  except:\n","    print(\"Error wrting to CSV\")\n","    return False"],"metadata":{"id":"LA_NWXnGgIPE","executionInfo":{"status":"ok","timestamp":1686289424988,"user_tz":420,"elapsed":203,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["aup_data = readInFile()\n","url_snapshots_dict = collectURL(aup_data)\n","writeToCSV(url_snapshots_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kenMwq05qobt","executionInfo":{"status":"ok","timestamp":1686289520931,"user_tz":420,"elapsed":69247,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}},"outputId":"3304b830-7df6-4d69-c309-a00986d46fa1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Exception when dealing with https://www.egress.com/legal/acceptable-use-policy\n","[Errno Expecting value] org.archive.util.io.RuntimeIOException: org.archive.wayback.exception.AdministrativeAccessControlException: Blocked Site Error\n",": 0\n","Exception when dealing with https://www.contractscounsel.com/t/us/acceptable-use-policy\n","HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /cdx/search/cdx?url=https://www.contractscounsel.com/t/us/acceptable-use-policy&fl=timestamp,digest,length,original&output=json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f631d4f59c0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Scrap AUP content"],"metadata":{"id":"isaq5MI0g9Ce"}},{"cell_type":"code","source":["# Now, we start to get AUP content using the urls\n","def scrapAUP(url_snapshots_dict):\n","  idx = 0\n","\n","  for url, tuples in url_snapshots_dict.items():\n","    if len(tuples) == 0:\n","      continue\n","    years = [t[0] for t in tuples]\n","    snapshots = [t[1] for t in tuples]\n","    for i in range(len(snapshots)):\n","      fname = f\"aup/{idx}-{years[i]}.txt\"\n","      data.get_aup_content(snapshots[i], fname)\n","    \n","    idx += 1\n"],"metadata":{"id":"VrS00UKqnzap","executionInfo":{"status":"ok","timestamp":1686289919833,"user_tz":420,"elapsed":214,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["scrapAUP(url_snapshots_dict)"],"metadata":{"id":"Gv-Lte_yq_FG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!tar -zcvf aup.tar.gz aup"],"metadata":{"id":"paLNVPV-9U8A","executionInfo":{"status":"aborted","timestamp":1686289385633,"user_tz":420,"elapsed":7,"user":{"displayName":"Siwei Huang","userId":"06197354294543149678"}}},"execution_count":null,"outputs":[]}]}