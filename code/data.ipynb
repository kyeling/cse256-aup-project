{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"aQGLVHVBAlkV"},"source":["## Part 0: Set-up"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22778,"status":"ok","timestamp":1684648915052,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"02kQ2WRyApO8","outputId":"2daf71eb-2347-47d4-f5b1-b7650296eb82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google) (2.4.1)\n"]}],"source":["import requests\n","import os\n","import gzip\n","import pandas as pd\n","import random\n","from googlesearch import search\n","from tqdm import tqdm\n","import csv \n","from bs4 import BeautifulSoup\n","from PyPDF2 import PdfReader\n","from io import BytesIO\n","\n","random.seed(0)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E_Kk8RoqHLeO"},"source":["## Part 1: Initial data collection\n","Download the data (and extract a random sample of urls).\n","\n","Data comes from the CrUX dataset. More information can be found at https://github.com/zakird/crux-top-lists"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1684648919224,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"-diWAtDu7Af5"},"outputs":[],"source":["# download crux dataset (if needed) and read into a dataframe\n","def get_dataset(fname='current.csv'):\n","  if not os.path.isfile(fname):\n","    print('downloading data...')\n","    url = 'https://raw.githubusercontent.com/zakird/crux-top-lists/main/data/global/current.csv.gz'\n","    response = requests.get(url)\n","\n","    # write download contents to 'current.csv.gz'\n","    gzip.open(f'{fname}.gz', 'wb').write(response.content)\n","    # unzip to get 'current.csv'\n","    os.system(f'gzip -d {fname}.gz')\n","\n","  try:\n","    df = pd.read_csv(fname, compression='gzip')\n","  except:\n","    df = pd.read_csv(fname)\n","  return df\n","\n","# returns a new dataframe of only websites with specified rank\n","def rank_filter(df, rank):\n","  return df[df['rank'] == rank]"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2730,"status":"ok","timestamp":1684648927884,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"VdIkfEMX5pS8","outputId":"58a65cc2-557a-4143-da89-bb602cfddbbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading data...\n","         index                               origin     rank\n","0            0                https://www.globo.com     1000\n","1            1          https://www.rightmove.co.uk     1000\n","2            2                   https://mobcup.net     1000\n","3            3           https://www.sahibinden.com     1000\n","4            4                https://tamilyogi.dog     1000\n","...        ...                                  ...      ...\n","999995  999995  https://www.studysmartwithchris.com  1000000\n","999996  999996  https://www.radiomarcabarcelona.com  1000000\n","999997  999997               https://www.vehikit.fr  1000000\n","999998  999998            https://www.zoomtekno.com  1000000\n","999999  999999             https://taujenudvaras.lt  1000000\n","\n","[1000000 rows x 3 columns]\n"]}],"source":["# create dataframe\n","df = get_dataset().reset_index()\n","ranks = df['rank'].unique()\n","print(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PW76RP_XHQRu"},"source":["## Part 2: Searching for AUPs"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1684651857855,"user":{"displayName":"Kyeling Ong","userId":"17310599495446242134"},"user_tz":420},"id":"YCAjQEKorFLf"},"outputs":[],"source":["# try removing trivial components of url\n","# for simplicity, any component consisting of 3 or fewer characters\n","def nontrivial_cmp(orig_url, aup_url): # condition function can only take one argument\n","  len3 = lambda s : len(s) > 3\n","  len3_and_orig = lambda s: len3(s) and s in orig_parts\n","\n","  orig_parts = list(filter(len3, orig_url.replace('https://', '').split('.')))\n","  aup_parts = list(filter(len3_and_orig, aup_url.replace('https://', '').split('.')))\n","  return bool(aup_parts) and ('aup' in aup_url or 'acceptable-use-policy' in aup_url)\n","\n","# take top google search results and see if any contain an aup\n","# search-width: n\n","def get_search_results(url, n=5):\n","  search_str = f'{url} acceptable use policy'\n","  for result in search(search_str, num=n, stop=n, pause=0): # NOTE removing pause may lead to 429 Too Many Requests\n","    if nontrivial_cmp(url, result):\n","      return result\n","  return None\n","\n","# get aup's in the bucket specified by a given rank\n","# don't worry about duplicates here, remove them later in a dataframe\n","def get_aups_in_bucket(partial_df):\n","  aups = []\n","  rank = partial_df.iloc[0]['rank']\n","\n","  # create a new file to store aup urls\n","  with open(f'aups-rank{rank}.csv', 'w') as f:\n","    w = csv.writer(f)\n","    w.writerow(['index', 'aup'])\n","\n","  for i in tqdm(range(len(partial_df))):\n","    row = partial_df.iloc[i]\n","    idx = row['index']\n","    url = row['origin']\n","       \n","    # run google search for relevant aup\n","    result = get_search_results(url)\n","    if result:\n","      aups.append((idx, result))\n","    \n","    # print progress update and write to file after every 1000 searches\n","    if i % 1000 == 0 and i != 0:\n","      print(f' found {len(aups)} new aups')\n","      with open(f'aups-rank{rank}.csv', 'a') as f:\n","        w = csv.writer(f)\n","        w.writerows(aups)\n","      aups = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TdK9KQaXCn0","outputId":"ea5b7694-5e69-4bbd-c039-1ed757f92d73"},"outputs":[],"source":["# run everything up to rank 50,000 (first four buckets) ...\n","rank_df = rank_filter(df, 5000)\n","get_aups_in_bucket(rank_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DYfiE0A0_0Ri"},"source":["## Part 3: Longitudinal data\n","Use the Wayback Machine API (https://archive.org/help/wayback_api.php) or the standalone Wayback CDX Server API (https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server#field-order) to query snapshots of each url."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MWcjbKO_y-5"},"outputs":[],"source":["# query the Wayback CDX Server API\n","# this provides more complex support, including returning all available snapshots\n","def query_cdx(url):\n","  query = f'http://web.archive.org/cdx/search/cdx?url={url}&fl=timestamp,digest,length,original&output=json'\n","  response = requests.get(query)\n","  return response.json()\n","\n","# get urls of all wayback machine snapshots from a current url\n","def get_snapshots(url):\n","  cdx = query_cdx(url)[1:] # skip header\n","\n","  # c[0] is timestamp and c[3] is original url\n","  snapshots = [f'http://web.archive.org/web/{c[0]}/{c[3]}' for c in cdx]\n","  timestamps = [c[0] for c in cdx]\n","  return snapshots, timestamps"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s41X1WBM3QkK"},"source":["## Part 4: Scraping content from URLs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bioP2tibB2Ps"},"outputs":[],"source":["def get_aup_content(aup_url, index, timestamp='current'):\n","  response = requests.get(aup_url)\n","  padded_index = str(index).zfill(6)\n","  text = \"\"\n","\n","  if 'text/html' in response.headers['Content-Type']:\n","    soup = BeautifulSoup(response.content)\n","    elems_to_rm = ['style', 'script', 'head', 'title', 'nav', 'heater', 'footer', 'button', 'a']\n","    for s in soup(elems_to_rm):\n","      s.extract()\n","    text = soup.get_text()\n","\n","  elif 'pdf' in response.headers['Content-Type']:\n","    # src: https://wellsr.com/python/read-pdf-files-with-python-using-pypdf2/\n","    pdf_bytes = BytesIO(response.content)\n","    pdf = PdfReader(pdf_bytes)\n","    for page in pdf.pages:\n","      text += page.extract_text()\n","    \n","  else:\n","    print('urecognized content type', response.headers['Content-Type'])\n","    # TODO return error\n","\n","  # strip leading/trailing space and drop blank lines\n","  lines = (line.strip() for line in text.splitlines())\n","  cleantext = '\\n'.join(line for line in lines if line)\n","\n","  # write to file\n","  open(f'./{padded_index}-{timestamp}.txt', 'w').write(cleantext)"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
